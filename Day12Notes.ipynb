{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day12Notes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urness/CS167Code/blob/main/Day12Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8ZtDAuV1pkR"
      },
      "source": [
        "# Day 12 Notes: Random Forests\n",
        "\n",
        "Using a new dataset, this set of notes walks through how to use Random Forests in sklearn, and also demonstrates tuning different parameters related to Random Forests. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr2RqT5F1llk"
      },
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "drive.mount('/content/drive')\n",
        "data= pd.read_csv('/content/drive/MyDrive/CS167Spring22/datasets/breast-cancer-wisconsin-data.csv')\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45PkceoVyFDL"
      },
      "source": [
        "# Notice that the 'Unnamed: 32' column is full of NaN... we need to get rid of it. \n",
        "data.drop('Unnamed: 32', inplace=True, axis=1)\n",
        "data.isna().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUCdUIbv15kX"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "target = \"diagnosis\"\n",
        "predictors = data.columns.drop(target) #gets all of the columns except the target\n",
        "\n",
        "train_data, test_data, train_sln, test_sln = train_test_split(data[predictors], data[target], test_size = 0.2, random_state=41)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's set up a single tree for comparison"
      ],
      "metadata": {
        "id": "12rRjbq0RzJq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf3h23iI2m6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbc40401-8ef0-4cf2-ee68-3902a29d6946"
      },
      "source": [
        "from sklearn import tree\n",
        "from sklearn import metrics\n",
        "\n",
        "# Let's use a single tree for comparison\n",
        "# a default Decision Tree Classifier\n",
        "\n",
        "dt = tree.DecisionTreeClassifier(random_state = 0)\n",
        "dt.fit(train_data,train_sln)\n",
        "predictions = dt.predict(test_data)\n",
        "\n",
        "print(\"accuracy score: \", metrics.accuracy_score(test_sln,predictions))\n",
        "vals = data[target].unique() ## possible classification values (M = malignant; B = benign)\n",
        "conf_mat = metrics.confusion_matrix(test_sln, predictions, labels=vals)\n",
        "print(pd.DataFrame(conf_mat, index = \"True \" + vals, columns = \"Predicted \" + vals))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy score:  0.956140350877193\n",
            "        Predicted M  Predicted B\n",
            "True M           39            1\n",
            "True B            4           70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5iXyptv3GSF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a055cca2-221a-4d50-c55e-7635f40efbca"
      },
      "source": [
        "# a Random Forest Classifier\n",
        "forest = RandomForestClassifier(random_state = 0, n_estimators=100)\n",
        "forest.fit(train_data,train_sln)\n",
        "predictions = forest.predict(test_data)\n",
        "print(\"accuracy score: \", metrics.accuracy_score(test_sln,predictions))\n",
        "\n",
        "vals = data[target].unique() ## possible classification values (M = malignant; B = benign)\n",
        "conf_mat = metrics.confusion_matrix(test_sln, predictions, labels=vals)\n",
        "print(pd.DataFrame(conf_mat, index = \"True \" + vals, columns = \"Predicted \" + vals))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy score:  0.9824561403508771\n",
            "        Predicted M  Predicted B\n",
            "True M           40            0\n",
            "True B            2           72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise #1\n",
        "Look at RandomForestClassifer Documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=randomforestclassifier#sklearn.ensemble.RandomForestClassifier)\n",
        "\n",
        "- What is the default number of trees?\n",
        "- How does increasing or decreasing the number of trees effect accuracy?"
      ],
      "metadata": {
        "id": "RQeByt3_sdaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise #2\n",
        "Can you improve upon the Breast cancer diagnosis accuracy?\n",
        "Can you get 100% accuracy? Why do you think so?\n"
      ],
      "metadata": {
        "id": "bMl_cp3Cvp1i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-hqyWEN3h1R"
      },
      "source": [
        "## Feature Importances\n",
        "Becuase we are building so many small decision trees in a random forest, we have the added benefit of being able to see what features are most commonly used as high information gain features. The code below shows how we can plot the 'Feature Importance' chart for a random forest. \n",
        "\n",
        "In this particular run, it looks like perimeter_worst was the most important feature, but there were a handful of others that were important as well. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ctkby7Ni3Zxp"
      },
      "source": [
        "# It looks like our random forest model achieved pretty good accuracy. \n",
        "# Now lets check how important each of the features was in the ensemble of models we built.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "index = range(len(predictors)) #creates a list of numbers the right size to use as the index\n",
        "\n",
        "plt.figure(figsize=(8,10)) #making the table a bit bigger so the text is readable\n",
        "plt.barh(index,forest.feature_importances_,height=0.8) #horizontal bar chart\n",
        "plt.ylabel('Feature')\n",
        "plt.yticks(index,predictors) #put the feature names at the y tick marks\n",
        "plt.xlabel(\"Random Forest Feature Importance\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8oYvTIf3d6C"
      },
      "source": [
        "#This function just loops through a series of n_estimator values, builds a different model\n",
        "#for each, and then plots their respective accuracies. By making it a function, it's easier\n",
        "#to try out different ranges of numbers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def tune_number_of_trees(n_estimator_values):\n",
        "    rf_accuracies = []\n",
        "\n",
        "    for n in n_estimator_values:\n",
        "\n",
        "        curr_rf = RandomForestClassifier(n_estimators=n, random_state=41)\n",
        "        curr_rf.fit(train_data,train_sln)\n",
        "        curr_predictions = curr_rf.predict(test_data)\n",
        "        curr_accuracy = metrics.accuracy_score(test_sln,curr_predictions)\n",
        "        rf_accuracies.append(curr_accuracy)\n",
        "\n",
        "\n",
        "    plt.suptitle('Random Forest accuracy vs. number of trees',fontsize=18)\n",
        "    plt.xlabel('# trees')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.plot(n_estimator_values,rf_accuracies,'ro-')\n",
        "    plt.axis([0,n_estimator_values[-1]+1,.8,1])\n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "tune_number_of_trees(range(1,31))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeZbed0331ao"
      },
      "source": [
        "\n",
        "It looks like whether we are using small numbers of trees or large ones, the accuracy stays about the same. It appears at least sometimes that Random Forest doesn't take a lot of tuning of the number of trees.\n",
        "\n",
        "Now let's try tuning the number of features used with each tree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQKvSCMM3vsz"
      },
      "source": [
        "def tune_max_features(max_features_values):\n",
        "    rf_accuracies = []\n",
        "\n",
        "    for m in max_features_values:\n",
        "\n",
        "        curr_rf = RandomForestClassifier(n_estimators=10,max_features=m, random_state=31)\n",
        "        curr_rf.fit(train_data,train_sln)\n",
        "        curr_predictions = curr_rf.predict(test_data)\n",
        "        curr_accuracy = metrics.accuracy_score(test_sln,curr_predictions)\n",
        "        rf_accuracies.append(curr_accuracy)\n",
        "\n",
        "\n",
        "    plt.suptitle('Random Forest accuracy vs. max features',fontsize=18)\n",
        "    plt.xlabel('max features')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.plot(max_features_values,rf_accuracies,'ro-')\n",
        "    plt.axis([0,max_features_values[-1]+1,.8,1])\n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "tune_max_features(range(1,11))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHTFIV8W4FE9"
      },
      "source": [
        "Note that the above could be subject to changes based on the initial random_state.\n",
        "\n",
        "For this data, which is apparently very easy to learn on (accuracy is very high), the number of features used with each tree also didn't matter much when used with an ensemble of 10 trees. This is probably something worth tuning if you have a lot of features, especially if many of them might not be very relevant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbTzYfmxzfXu"
      },
      "source": [
        "# Exercise #3 \n",
        "Apply random forest to the wine dataset (Notebook4)\n",
        "Can you get an R2 score above .55?\n",
        "\n",
        "- **hint** Search SciKit Learn [website](https://scikit-learn.org/) for 'RandomForestRegressor'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bhg_5SYEwgyA"
      },
      "source": [
        "import sklearn\n",
        "import pandas\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn import metrics\n",
        "\n",
        "wine = pandas.read_csv('/content/drive/MyDrive/CS167Spring22/datasets/winequality-white.csv') # change this to match your dataset directory\n",
        "\n",
        "target= 'quality'\n",
        "predictors = wine.columns.drop(target)\n",
        "train_data, test_data, train_sln, test_sln = train_test_split(wine[predictors], wine[target], test_size = 0.2, random_state=41)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}