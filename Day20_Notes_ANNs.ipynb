{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day20_Notes_ANNs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urness/CS167Code/blob/main/Day20_Notes_ANNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PSKyhdVBMBP"
      },
      "source": [
        "# Day 20 Code: Artificial Neural Networks\n",
        "\n",
        "We're going to start off by using sklearn MLP to implement a multilayer perceptron, and then we're going to use a deep learning framework, Tensorflow with Keras to build a neural network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlQnxuSxBGwV",
        "outputId": "421e211c-ad07-49a9-e7ab-2289dc18da30"
      },
      "source": [
        "from google.colab import drive\n",
        "import pandas\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYGVt9K5DcjG"
      },
      "source": [
        "data = pandas.read_csv('/content/drive/MyDrive/CS167Spring22/datasets/irisData.csv')\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPcbE8vaBLJW"
      },
      "source": [
        "import pandas\n",
        "import numpy\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Split the dataset\n",
        "predictors = data.columns.drop('species')\n",
        "target = \"species\"\n",
        "train_data, test_data, train_sln, test_sln = train_test_split(data[predictors], data[target], test_size = 0.2, random_state=41)\n",
        "\n",
        "#Normalize Data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(train_data)\n",
        "train_data = scaler.transform(train_data)\n",
        "test_data = scaler.transform(test_data)\n",
        "train_data[0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOm5Qh1_BzVx"
      },
      "source": [
        "## Build out a Multilayer Perceptron using Scikit-Learn:\n",
        "Here are the links to the documentation: \n",
        "- [sklearn.neural_network.MLPRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html)\n",
        "- [sklearn.neural_network.MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)\n",
        "\n",
        "I'm providing some code below to create a pretty confusion matrix. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3XaPu9yEQAd"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "pretty = True\n",
        "#This function will print a confusion matrix\n",
        "# It takes the test_slns, the preds, and a boolean variable pretty, which when True will print a prettier confusion matrix and if it's false it will print a standard conf matrix.\n",
        "def print_confusion_matrix(test_sln, preds, pretty):\n",
        "  cf_matrix = confusion_matrix(test_sln, preds,)\n",
        "  if pretty:\n",
        "    sns.heatmap(cf_matrix, annot=True,  xticklabels=['p_setosa', 'p_versicolor', 'p_virginica'], yticklabels=['t_setosa', 't_versicolor', 't_virginica']) #p for predicted, t for true\n",
        "  else:\n",
        "    print(cf_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u63K8A1mBhOS"
      },
      "source": [
        "# Set up MLP\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "mlp = MLPClassifier(random_state=0,hidden_layer_sizes = (100,), max_iter = 800)\n",
        "mlp.fit(train_data,train_sln)\n",
        "predictions = mlp.predict(test_data)\n",
        "\n",
        "print(\"Accuracy: \", metrics.accuracy_score(test_sln,predictions))\n",
        "\n",
        "#call the function from above to get a pretty conf_mat:\n",
        "print_confusion_matrix(test_sln, predictions, True)\n",
        "\n",
        "# or:\n",
        "vals = data[target].unique() ## possible classification values (species)\n",
        "conf_mat = metrics.confusion_matrix(test_sln, predictions, labels=vals)\n",
        "print(pandas.DataFrame(conf_mat, index = \"True \" + vals, columns = \"Pred \" + vals))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7QVdyQfDLqZ"
      },
      "source": [
        "## In-Class Exercise:\n",
        "\n",
        "1. Read in the Boston Housing dataset\n",
        "2. Normalize your data\n",
        "3. Use a [MLPRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html) to predict the price of a house 'MEDV'\n",
        "4. Play around with changing the parameters, see what the best R2 score you can get is. \n",
        "\n",
        "Be ready to report what your best r2 score is and what parameters you used/changed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uR0BTwTgBucW"
      },
      "source": [
        "# Your code goes here for the In-Class Exercise\n",
        "# hint: let's use random_state = 0\n",
        "# train_data, test_data, train_sln, test_sln = train_test_split(data[predictors], data[target], test_size = 0.2, random_state=0)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaB9j0ZcdaDd"
      },
      "source": [
        "# Introducing Deep Learning Frameworks\n",
        "\n",
        "Go ahead and go up to 'Runtime', and select 'change runtime type' from the dropdown list, select 'GPU'. If you complete this step correctly, the following code should say `Found GPU at: /device:GPU:0` or something similar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVu_afildpIY",
        "outputId": "f30ea3b8-1121-4ee9-ce87-667e36d5e019"
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bA6K3nymJsg"
      },
      "source": [
        "# Iris Dataset with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kogXVGmkmMNj",
        "outputId": "f84271af-b7c7-45ce-c0fd-66f63f0b1b7b"
      },
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "import numpy\n",
        "\n",
        "# we're going to use the iris dataset, but load it from sklearn \n",
        "iris = load_iris()\n",
        "X = iris['data']\n",
        "y = iris['target']\n",
        "names = iris['target_names']\n",
        "feature_names = iris['feature_names']\n",
        "\n",
        "# One hot encoding\n",
        "enc = OneHotEncoder()\n",
        "Y = enc.fit_transform(y[:, numpy.newaxis]).toarray()\n",
        "\n",
        "#normalize our data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data set into training and testing\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    X_scaled, Y, test_size=0.5, random_state=2)\n",
        "\n",
        "#X is whre we store our \n",
        "print(X_train[0,:])\n",
        "print(Y_train[0,:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.06866179 -0.13197948  0.25122143  0.3957741 ]\n",
            "[0. 1. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfvZyvC5ombh"
      },
      "source": [
        "#build our model\n",
        "n_features = X.shape[1] #X generally stands for our predictors\n",
        "n_classes = Y.shape[1] #Y generally stands for our target\n",
        "\n",
        "model = Sequential(name='iris_1')\n",
        "model.add(Dense(64, input_dim=n_features, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(n_classes, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='categorical_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2RSJESIoH4G",
        "outputId": "88677b99-5be8-4b08-df8f-873e3f991260"
      },
      "source": [
        "from keras.callbacks import TensorBoard\n",
        "# TensorBoard Callback\n",
        "cb = TensorBoard()\n",
        "\n",
        "history_dict = {}\n",
        "history_callback = model.fit(X_train, Y_train,\n",
        "                              batch_size=5,\n",
        "                              epochs=50,\n",
        "                              verbose=0,\n",
        "                              validation_data=(X_test, Y_test),\n",
        "                              callbacks=[cb])\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "history_dict[model.name] = [history_callback, model]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 1.2927358150482178\n",
            "Test accuracy: 0.8933333158493042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW_tE14Sl8Gy"
      },
      "source": [
        "# Boston Housing Dataset with Keras\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_WEo0d1d1BJ"
      },
      "source": [
        "from keras.datasets import boston_housing\n",
        "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYSZCeDTeKlm"
      },
      "source": [
        "# take a look at the data\n",
        "\n",
        "print(f'Training data : {train_data.shape}')\n",
        "print(f'Test data : {test_data.shape}')\n",
        "print(f'Training sample : {train_data[0]}')\n",
        "print(f'Training target sample : {train_targets[0]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUXl-jTqeNhs"
      },
      "source": [
        "#Let's normalize the data.  (This calculates the z-score for the traning data)\n",
        "mean = train_data.mean(axis=0)\n",
        "train_data -= mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data /= std\n",
        "\n",
        "test_data -= mean\n",
        "test_data /= std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIY3PFdQecxg"
      },
      "source": [
        "#Build the network\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential(name='boston_housing1')   #initialize the model\n",
        "\n",
        "#add some layers. Dense is a fully connected layer\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(train_data.shape[1],)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "\n",
        "#we only want one value as an ouput, so our last layer has a Dense layer with 1 neuron.\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "#now we 'compile' our model by specifying what kind of optimizer we will use to train. \n",
        "#In this case, we are using 'rmsprop' and \n",
        "model.compile(optimizer='rmsprop',\n",
        "          loss='mse',\n",
        "          metrics=['mae'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4hnXfHlfDg-",
        "outputId": "d57e92e7-d708-4ceb-ff34-d4ec78bebadd"
      },
      "source": [
        "\n",
        "model.fit(train_data, train_targets, epochs=80, batch_size=16, verbose=0)\n",
        "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)\n",
        "print('MSE:', test_mse_score)\n",
        "print('MAE:', test_mae_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 4ms/step - loss: 18.6239 - mae: 2.6347\n",
            "MSE: 18.623865127563477\n",
            "MAE: 2.634739398956299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7WVqBhjfwGW"
      },
      "source": [
        "# In Class Exercise #2\n",
        "What parameters from the models above do you think you can/should change? \n",
        "\n",
        "\n",
        "Try these\n",
        "- Change the number of neruons in each layer.  \n",
        "- Add a layer to the model.\n",
        "- Change the activation function of the model, [here is the documentation](https://keras.io/api/layers/activations/)\n",
        "- Change the optimizer, [here is the documentation](https://keras.io/api/optimizers/) with a list of options\n",
        "- look at the [metrics](https://keras.io/api/metrics/) try adding another metric.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bU_2qYHgjjn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}